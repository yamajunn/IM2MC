{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# 各フォルダパス\n",
    "skins_dir = 'C:/Users/Owner/Desktop/archive/Skins'\n",
    "missing_dir = 'C:/Users/Owner/Desktop/archive/Missing'\n",
    "masks_dir = 'C:/Users/Owner/Desktop/archive/Masks'\n",
    "\n",
    "def load_image(path, channels=4):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=channels)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1]に正規化\n",
    "    return image\n",
    "\n",
    "def load_sample(file_name):\n",
    "    # file_name は Tensor で、ファイル名のみが入っている前提\n",
    "    skin_path = tf.strings.join([skins_dir, file_name], separator='/')\n",
    "    \n",
    "    missing_file = tf.strings.join([\"missing_\", file_name])\n",
    "    missing_path = tf.strings.join([missing_dir, missing_file], separator='/')\n",
    "    \n",
    "    mask_file = tf.strings.join([\"mask_\", file_name])\n",
    "    mask_path = tf.strings.join([masks_dir, mask_file], separator='/')\n",
    "    \n",
    "    skin = load_image(skin_path, channels=4)\n",
    "    missing = load_image(missing_path, channels=4)\n",
    "    mask = load_image(mask_path, channels=1)  # マスクは1チャネル\n",
    "    \n",
    "    # 入力は欠損画像とマスクをチャネル方向に連結 (shape: (64, 64, 5))\n",
    "    input_image = tf.concat([missing, mask], axis=-1)\n",
    "    return input_image, skin\n",
    "\n",
    "# データセットのサイズ\n",
    "total = 1000\n",
    "\n",
    "# 全てのスキン画像のファイルリストを取得\n",
    "file_names = tf.data.Dataset.list_files(os.path.join(skins_dir, '*.png')).shuffle(buffer_size=total)\n",
    "file_names = file_names.take(total)\n",
    "\n",
    "# ファイル名の抽出とサンプル作成\n",
    "dataset = file_names.map(lambda fn: load_sample(tf.strings.split(fn, os.sep)[-1]))\n",
    "dataset = dataset.shuffle(buffer_size=total)\n",
    "\n",
    "# 全体の件数が1000件の場合、80%をトレーニング、20%を検証に利用\n",
    "train_size = int(total * 0.8)\n",
    "# トレーニングデータと検証データに分割\n",
    "train_dataset = dataset.take(train_size).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset   = dataset.skip(train_size).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義 (U-Net)\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, activation='leaky_relu'):\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # LeakyReLU による活性化\n",
    "    if activation == 'leaky_relu':\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    else:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(x, filters):\n",
    "    conv = conv_block(x, filters)\n",
    "    conv = conv_block(conv, filters)\n",
    "    pool = layers.MaxPooling2D((2, 2))(conv)\n",
    "    return conv, pool\n",
    "\n",
    "def decoder_block(x, skip, filters):\n",
    "    up = layers.UpSampling2D((2, 2))(x)\n",
    "    up = layers.Concatenate()([up, skip])\n",
    "    conv = conv_block(up, filters)\n",
    "    conv = conv_block(conv, filters)\n",
    "    return conv\n",
    "\n",
    "def build_unet(input_shape=(64, 64, 5)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    s1, p1 = encoder_block(inputs, 32)\n",
    "    s2, p2 = encoder_block(p1, 64)\n",
    "    \n",
    "    # Bottleneck\n",
    "    b = conv_block(p2, 128)\n",
    "    b = conv_block(b, 128)\n",
    "    \n",
    "    # Decoder\n",
    "    d2 = decoder_block(b, s2, 64)\n",
    "    d1 = decoder_block(d2, s1, 32)\n",
    "    \n",
    "    # 出力層：RGBA出力のため4チャネル、値は [0,1] 範囲 (sigmoid)\n",
    "    outputs = layers.Conv2D(4, (1, 1), padding='same', activation='sigmoid')(d1)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras import backend as K\n",
    "# import tensorflow as tf\n",
    "\n",
    "# def conv_block(x, filters, kernel_size=3, activation='leaky_relu'):\n",
    "#     x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     # LeakyReLU による活性化\n",
    "#     if activation == 'leaky_relu':\n",
    "#         x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "#     else:\n",
    "#         x = layers.Activation(activation)(x)\n",
    "#     return x\n",
    "\n",
    "# def self_attention_block(x, filters):\n",
    "#     # 入力のチャネル数を取得\n",
    "#     batch_size, height, width, channels = K.int_shape(x)\n",
    "    \n",
    "#     # 圧縮次元数\n",
    "#     f = filters // 8\n",
    "    \n",
    "#     # Query, Key, Value の生成\n",
    "#     query = layers.Conv2D(f, kernel_size=1, padding='same')(x)\n",
    "#     key = layers.Conv2D(f, kernel_size=1, padding='same')(x)\n",
    "#     value = layers.Conv2D(filters, kernel_size=1, padding='same')(x)\n",
    "    \n",
    "#     # 形状変換\n",
    "#     query_reshape = layers.Reshape((height * width, f))(query)\n",
    "#     key_reshape = layers.Reshape((height * width, f))(key)\n",
    "#     value_reshape = layers.Reshape((height * width, filters))(value)\n",
    "    \n",
    "#     # 転置操作\n",
    "#     key_transpose = layers.Permute((2, 1))(key_reshape)\n",
    "    \n",
    "#     # 注意スコア計算\n",
    "#     attention_score = layers.Dot(axes=(2, 1))([query_reshape, key_transpose])\n",
    "#     attention_score = layers.Lambda(lambda x: x / K.sqrt(tf.cast(f, tf.float32)))(attention_score)\n",
    "#     attention_weights = layers.Activation('softmax')(attention_score)\n",
    "    \n",
    "#     # 注意マップを適用\n",
    "#     context = layers.Dot(axes=(2, 1))([attention_weights, value_reshape])\n",
    "#     context = layers.Reshape((height, width, filters))(context)\n",
    "    \n",
    "#     # 残差接続のスケーリング係数\n",
    "#     gamma = layers.Conv2D(1, kernel_size=1, padding='same', use_bias=False)(x)\n",
    "#     gamma = layers.Reshape((height, width, 1))(gamma)\n",
    "    \n",
    "#     # 残差接続とスケーリング\n",
    "#     output = layers.Add()([x, layers.Multiply()([gamma, context])])\n",
    "    \n",
    "#     return output\n",
    "\n",
    "# def encoder_block(x, filters, use_attention=False):\n",
    "#     conv = conv_block(x, filters)\n",
    "#     conv = conv_block(conv, filters)\n",
    "    \n",
    "#     # 自己注意機構の適用（オプション）\n",
    "#     if use_attention:\n",
    "#         conv = self_attention_block(conv, filters)\n",
    "    \n",
    "#     pool = layers.MaxPooling2D((2, 2))(conv)\n",
    "#     return conv, pool\n",
    "\n",
    "# def decoder_block(x, skip, filters, use_attention=False):\n",
    "#     up = layers.UpSampling2D((2, 2))(x)\n",
    "#     up = layers.Concatenate()([up, skip])\n",
    "#     conv = conv_block(up, filters)\n",
    "#     conv = conv_block(conv, filters)\n",
    "    \n",
    "#     # 自己注意機構の適用（オプション）\n",
    "#     if use_attention:\n",
    "#         conv = self_attention_block(conv, filters)\n",
    "    \n",
    "#     return conv\n",
    "\n",
    "# def build_unet_with_attention(input_shape=(64, 64, 5)):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "#     # Encoder\n",
    "#     s1, p1 = encoder_block(inputs, 32, use_attention=False)\n",
    "#     s2, p2 = encoder_block(p1, 64, use_attention=True)\n",
    "    \n",
    "#     # Bottleneck\n",
    "#     b = conv_block(p2, 128)\n",
    "#     b = conv_block(b, 128)\n",
    "#     b = self_attention_block(b, 128)  # ボトルネックに自己注意機構を適用\n",
    "    \n",
    "#     # Decoder\n",
    "#     d2 = decoder_block(b, s2, 64, use_attention=True)\n",
    "#     d1 = decoder_block(d2, s1, 32, use_attention=False)\n",
    "    \n",
    "#     # 出力層：RGBA出力のため4チャネル、値は [0,1] 範囲 (sigmoid)\n",
    "#     outputs = layers.Conv2D(4, (1, 1), padding='same', activation='sigmoid')(d1)\n",
    "    \n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def l1_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def l2_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def edge_loss(y_true, y_pred):\n",
    "    sobel_true = tf.image.sobel_edges(y_true)\n",
    "    sobel_pred = tf.image.sobel_edges(y_pred)\n",
    "\n",
    "    # X方向のエッジ差分\n",
    "    edge_x_loss = tf.abs(sobel_true[..., 0] - sobel_pred[..., 0])\n",
    "    # Y方向のエッジ差分\n",
    "    edge_y_loss = tf.abs(sobel_true[..., 1] - sobel_pred[..., 1])\n",
    "\n",
    "    # 両方のエッジの差分の平均を損失として使う\n",
    "    return tf.reduce_mean(edge_x_loss + edge_y_loss)\n",
    "\n",
    "def mae_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    # SSIMは[0, 1]範囲の画像に適用されるため、出力を[0, 1]に正規化\n",
    "    y_true = (y_true + 1.0) / 2.0  # RGBA画像などの場合、[-1, 1]の範囲から[0, 1]に変換\n",
    "    y_pred = (y_pred + 1.0) / 2.0  # 同様に出力を[0, 1]に正規化\n",
    "    \n",
    "    # SSIMを計算\n",
    "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
    "\n",
    "def laplacian_filter(image):\n",
    "    \"\"\"RGBAの各チャンネルにラプラシアンフィルタを適用\"\"\"\n",
    "    laplacian_kernel = tf.constant([\n",
    "        [0,  1,  0],\n",
    "        [1, -4,  1],\n",
    "        [0,  1,  0]\n",
    "    ], dtype=tf.float32)\n",
    "    \n",
    "    laplacian_kernel = tf.reshape(laplacian_kernel, [3, 3, 1, 1])  # (高さ, 幅, 入力チャンネル, 出力チャンネル)\n",
    "    \n",
    "    # RGBAの各チャンネルに適用するためのフィルタを作成\n",
    "    filters = tf.tile(laplacian_kernel, [1, 1, 4, 1])  # (3, 3, 4, 4) に拡張\n",
    "\n",
    "    # 4次元テンソル (バッチ, 高さ, 幅, チャンネル) の形状を維持\n",
    "    image = tf.expand_dims(image, axis=0)  # バッチ次元を追加 (None, H, W, 4)\n",
    "    edges = tf.nn.conv2d(image, filters, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "    return tf.squeeze(edges)  # バッチ次元を削除\n",
    "\n",
    "def laplacian_loss(y_true, y_pred):\n",
    "    edge_true = laplacian_filter(y_true)\n",
    "    edge_pred = laplacian_filter(y_pred)\n",
    "\n",
    "    # L1 損失\n",
    "    loss = tf.reduce_mean(tf.abs(edge_true - edge_pred))\n",
    "    return loss\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    loss_l1 = l1_loss(y_true, y_pred)\n",
    "    # loss_edge = edge_loss(y_true, y_pred)\n",
    "    # loss_mae = mae_loss(y_true, y_pred)\n",
    "    # loss_mse = mse_loss(y_true, y_pred)\n",
    "    # loss_ssim = ssim_loss(y_true, y_pred)\n",
    "    loss_laplacian = laplacian_loss(y_true, y_pred)\n",
    "\n",
    "    return loss_l1 + 0.2 * loss_laplacian\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=total_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 6s 128ms/step - loss: 0.2650 - val_loss: 0.3781\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 111ms/step - loss: 0.1942 - val_loss: 0.3421\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 5s 106ms/step - loss: 0.1540 - val_loss: 0.2681\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 105ms/step - loss: 0.1240 - val_loss: 0.2185\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.1023 - val_loss: 0.2079\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 105ms/step - loss: 0.0894 - val_loss: 0.1763\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0817 - val_loss: 0.1540\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0753 - val_loss: 0.1422\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 111ms/step - loss: 0.0669 - val_loss: 0.1213\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 110ms/step - loss: 0.0646 - val_loss: 0.1066\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.0629 - val_loss: 0.1025\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 107ms/step - loss: 0.0603 - val_loss: 0.0975\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 108ms/step - loss: 0.0567 - val_loss: 0.0821\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0550 - val_loss: 0.0715\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0553 - val_loss: 0.0664\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0513 - val_loss: 0.0668\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 92ms/step - loss: 0.0502 - val_loss: 0.0549\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0506 - val_loss: 0.0562\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0488 - val_loss: 0.0528\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0474 - val_loss: 0.0497\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 93ms/step - loss: 0.0486 - val_loss: 0.0491\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0467 - val_loss: 0.0474\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0461 - val_loss: 0.0477\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.0443 - val_loss: 0.0458\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0446 - val_loss: 0.0417\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0424 - val_loss: 0.0441\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0448 - val_loss: 0.0443\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.0449 - val_loss: 0.0466\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0426 - val_loss: 0.0466\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 4s 105ms/step - loss: 0.0421 - val_loss: 0.0467\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 5s 111ms/step - loss: 0.0944 - val_loss: 0.1512\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0569 - val_loss: 0.1132\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0429 - val_loss: 0.1021\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0357 - val_loss: 0.0963\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0304 - val_loss: 0.0868\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.0278 - val_loss: 0.0788\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.0269 - val_loss: 0.0708\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0251 - val_loss: 0.0573\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 107ms/step - loss: 0.0238 - val_loss: 0.0544\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0223 - val_loss: 0.0498\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0226 - val_loss: 0.0444\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0213 - val_loss: 0.0387\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0209 - val_loss: 0.0393\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0204 - val_loss: 0.0313\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0203 - val_loss: 0.0301\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0174 - val_loss: 0.0282\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0188 - val_loss: 0.0233\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0184 - val_loss: 0.0195\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0187 - val_loss: 0.0192\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0174 - val_loss: 0.0191\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0179 - val_loss: 0.0177\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 103ms/step - loss: 0.0170 - val_loss: 0.0174\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.0165 - val_loss: 0.0196\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0165 - val_loss: 0.0149\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0167 - val_loss: 0.0186\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.0180 - val_loss: 0.0181\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 4s 93ms/step - loss: 0.0162 - val_loss: 0.0181\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 4s 110ms/step - loss: 0.0167 - val_loss: 0.0174\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0156 - val_loss: 0.0160\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 6s 112ms/step - loss: 0.6244 - val_loss: 0.7141\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.4681 - val_loss: 0.5771\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.4083 - val_loss: 0.5400\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.3716 - val_loss: 0.5359\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.3479 - val_loss: 0.4908\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.3369 - val_loss: 0.5032\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.3188 - val_loss: 0.4517\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 110ms/step - loss: 0.3095 - val_loss: 0.4395\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.2993 - val_loss: 0.3848\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.2876 - val_loss: 0.3867\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.2823 - val_loss: 0.3430\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.2775 - val_loss: 0.3282\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 116ms/step - loss: 0.2623 - val_loss: 0.3110\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.2518 - val_loss: 0.3024\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.2436 - val_loss: 0.2739\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.2431 - val_loss: 0.2677\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.2363 - val_loss: 0.2543\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.2413 - val_loss: 0.2342\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.2346 - val_loss: 0.2597\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.2272 - val_loss: 0.2179\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.2329 - val_loss: 0.2395\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.2224 - val_loss: 0.2249\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.2259 - val_loss: 0.2203\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.2214 - val_loss: 0.2196\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.2266 - val_loss: 0.2357\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 5s 102ms/step - loss: 0.2352 - val_loss: 0.3560\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.1625 - val_loss: 0.2935\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.1261 - val_loss: 0.2618\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.1022 - val_loss: 0.2300\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0871 - val_loss: 0.1799\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0795 - val_loss: 0.1825\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0709 - val_loss: 0.1412\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 106ms/step - loss: 0.0643 - val_loss: 0.1273\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0621 - val_loss: 0.1265\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 113ms/step - loss: 0.0615 - val_loss: 0.1247\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 103ms/step - loss: 0.0552 - val_loss: 0.0985\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 105ms/step - loss: 0.0547 - val_loss: 0.0867\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0542 - val_loss: 0.0742\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.0506 - val_loss: 0.0732\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0485 - val_loss: 0.0639\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0488 - val_loss: 0.0697\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0506 - val_loss: 0.0545\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0479 - val_loss: 0.0475\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.0456 - val_loss: 0.0534\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.0472 - val_loss: 0.0513\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 93ms/step - loss: 0.0437 - val_loss: 0.0474\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0462 - val_loss: 0.0502\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0452 - val_loss: 0.0435\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0448 - val_loss: 0.0480\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0457 - val_loss: 0.0431\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0428 - val_loss: 0.0451\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 4s 107ms/step - loss: 0.0443 - val_loss: 0.0490\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.0427 - val_loss: 0.0437\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0412 - val_loss: 0.0424\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0431 - val_loss: 0.0452\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0409 - val_loss: 0.0419\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.0406 - val_loss: 0.0425\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0411 - val_loss: 0.0474\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0405 - val_loss: 0.0422\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0409 - val_loss: 0.0446\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0411 - val_loss: 0.0469\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 5s 111ms/step - loss: 0.0922 - val_loss: 0.1650\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0520 - val_loss: 0.1309\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 4s 92ms/step - loss: 0.0396 - val_loss: 0.1094\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0344 - val_loss: 0.0851\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0297 - val_loss: 0.0674\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0274 - val_loss: 0.0687\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.0253 - val_loss: 0.0553\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0246 - val_loss: 0.0481\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 108ms/step - loss: 0.0234 - val_loss: 0.0463\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 93ms/step - loss: 0.0225 - val_loss: 0.0433\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.0214 - val_loss: 0.0432\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0217 - val_loss: 0.0313\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0202 - val_loss: 0.0264\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0191 - val_loss: 0.0273\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0194 - val_loss: 0.0244\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0183 - val_loss: 0.0218\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 92ms/step - loss: 0.0194 - val_loss: 0.0241\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 108ms/step - loss: 0.0166 - val_loss: 0.0203\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 109ms/step - loss: 0.0184 - val_loss: 0.0190\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0171 - val_loss: 0.0260\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 93ms/step - loss: 0.0176 - val_loss: 0.0228\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0179 - val_loss: 0.0179\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.0170 - val_loss: 0.0209\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 92ms/step - loss: 0.0174 - val_loss: 0.0192\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.0173 - val_loss: 0.0193\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.0171 - val_loss: 0.0190\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 4s 93ms/step - loss: 0.0178 - val_loss: 0.0192\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 6s 122ms/step - loss: 0.3679 - val_loss: 0.4529\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.2603 - val_loss: 0.4466\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 4s 109ms/step - loss: 0.2295 - val_loss: 0.4174\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.2143 - val_loss: 0.3881\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.1956 - val_loss: 0.3651\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.1763 - val_loss: 0.3263\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.1708 - val_loss: 0.3008\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 108ms/step - loss: 0.1626 - val_loss: 0.2809\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.1572 - val_loss: 0.2524\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.1471 - val_loss: 0.2488\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.1463 - val_loss: 0.2276\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.1433 - val_loss: 0.1792\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.1407 - val_loss: 0.1734\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.1369 - val_loss: 0.1494\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.1422 - val_loss: 0.1631\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 110ms/step - loss: 0.1328 - val_loss: 0.1461\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.1351 - val_loss: 0.1417\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.1328 - val_loss: 0.1338\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 103ms/step - loss: 0.1315 - val_loss: 0.1344\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.1282 - val_loss: 0.1301\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 103ms/step - loss: 0.1330 - val_loss: 0.1291\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.1370 - val_loss: 0.1265\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.1312 - val_loss: 0.1340\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.1300 - val_loss: 0.1337\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.1251 - val_loss: 0.1248\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 112ms/step - loss: 0.1262 - val_loss: 0.1199\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.1246 - val_loss: 0.1305\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.1209 - val_loss: 0.1220\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.1240 - val_loss: 0.1209\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.1244 - val_loss: 0.1272\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.1234 - val_loss: 0.1254\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 6s 106ms/step - loss: 0.3856 - val_loss: 0.5314\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.2599 - val_loss: 0.4708\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 4s 94ms/step - loss: 0.2094 - val_loss: 0.3860\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 4s 105ms/step - loss: 0.1806 - val_loss: 0.3193\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.1645 - val_loss: 0.2923\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 4s 106ms/step - loss: 0.1463 - val_loss: 0.2514\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 4s 97ms/step - loss: 0.1388 - val_loss: 0.2458\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 102ms/step - loss: 0.1323 - val_loss: 0.2249\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.1246 - val_loss: 0.2189\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.1219 - val_loss: 0.1971\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.1150 - val_loss: 0.1848\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.1103 - val_loss: 0.1643\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.1103 - val_loss: 0.1725\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.1062 - val_loss: 0.1500\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.1077 - val_loss: 0.1145\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.1043 - val_loss: 0.1213\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 105ms/step - loss: 0.1022 - val_loss: 0.1172\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0986 - val_loss: 0.1327\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 106ms/step - loss: 0.1017 - val_loss: 0.1091\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 4s 106ms/step - loss: 0.1011 - val_loss: 0.1020\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.0978 - val_loss: 0.1024\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 99ms/step - loss: 0.0979 - val_loss: 0.0985\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 4s 105ms/step - loss: 0.0987 - val_loss: 0.0997\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.0975 - val_loss: 0.0983\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 4s 118ms/step - loss: 0.0966 - val_loss: 0.0937\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 110ms/step - loss: 0.0922 - val_loss: 0.1019\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 4s 108ms/step - loss: 0.0920 - val_loss: 0.0985\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0898 - val_loss: 0.0931\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 4s 96ms/step - loss: 0.0926 - val_loss: 0.0951\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 4s 98ms/step - loss: 0.0917 - val_loss: 0.0927\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0916 - val_loss: 0.0894\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 4s 100ms/step - loss: 0.0924 - val_loss: 0.0852\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 4s 101ms/step - loss: 0.0922 - val_loss: 0.0993\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 4s 95ms/step - loss: 0.0911 - val_loss: 0.0945\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 4s 108ms/step - loss: 0.0880 - val_loss: 0.0913\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 4s 103ms/step - loss: 0.0889 - val_loss: 0.0896\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 4s 104ms/step - loss: 0.0856 - val_loss: 0.0891\n"
     ]
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=l1_loss,\n",
    ")\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n",
    "model.save('models/l1_loss.h5')\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=l2_loss,\n",
    ")\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n",
    "model.save('models/l2_loss.h5')\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=edge_loss,\n",
    ")\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n",
    "model.save('models/edge_loss.h5')\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=mae_loss,\n",
    ")\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n",
    "model.save('models/mae_loss.h5')\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=mse_loss,\n",
    ")\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n",
    "model.save('models/mse_loss.h5')\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=ssim_loss,\n",
    ")\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n",
    "model.save('models/ssim_loss.h5')\n",
    "\n",
    "model = build_unet(input_shape=(64, 64, 5))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=total_loss,\n",
    ")\n",
    "model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n",
    "model.save('models/total_loss.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - ETA: 0s - loss: 0.2845"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TensorBoardやチェックポイントの設定\u001b[39;00m\n\u001b[0;32m      2\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:986\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m    985\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds, inner_filtered_flat_args):\n\u001b[0;32m    990\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TensorBoardやチェックポイントの設定\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "          validation_data=val_dataset,\n",
    "          epochs=50,\n",
    "          callbacks=[early_stop],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 学習履歴の取得\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# グラフの描画\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('U_NET/UNET.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "missing_image_path = 'Skins/0_missing.png'\n",
    "mask_image_path = 'Skins/0_mask.png'\n",
    "\n",
    "def load_image(path, channels=4):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=channels)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1]に正規化\n",
    "    return image\n",
    "\n",
    "missing = load_image(missing_image_path, channels=4)\n",
    "mask = load_image(mask_image_path, channels=1)\n",
    "\n",
    "missing = missing * mask\n",
    "\n",
    "# 入力は欠損画像とマスクをチャネル方向に連結\n",
    "input_image = tf.concat([missing, mask], axis=-1)\n",
    "\n",
    "# バッチ次元を追加\n",
    "input_image = tf.expand_dims(input_image, 0)\n",
    "\n",
    "# 保存された学習済みモデルを読み込む\n",
    "# model = load_model('U_NET/UNET.h5')\n",
    "model = load_model('U_NET/UNET.h5', custom_objects={'total_loss': total_loss})\n",
    "\n",
    "predicted_image = model.predict(input_image)[0]\n",
    "\n",
    "# 予測結果を表示\n",
    "\n",
    "plt.figure(figsize=(6, 6)).set(facecolor='gray')\n",
    "plt.imshow(predicted_image)\n",
    "plt.title('Predicted Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利用可能なフォント:\n",
      "['C:\\\\Windows\\\\Fonts\\\\javatext.ttf', 'C:\\\\Windows\\\\Fonts\\\\gadugib.ttf', 'C:\\\\Windows\\\\Fonts\\\\LeelawUI.ttf', 'C:\\\\Windows\\\\Fonts\\\\mmrtextb.ttf', 'C:\\\\Windows\\\\Fonts\\\\corbelli.ttf', 'C:\\\\Windows\\\\Fonts\\\\Gabriola.ttf', 'C:\\\\Windows\\\\Fonts\\\\malgun.ttf', 'C:\\\\Windows\\\\Fonts\\\\tahoma.ttf', 'C:\\\\Windows\\\\Fonts\\\\malgunbd.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoeuiz.ttf', 'C:\\\\Windows\\\\Fonts\\\\consola.ttf', 'C:\\\\Windows\\\\Fonts\\\\pala.ttf', 'C:\\\\Windows\\\\Fonts\\\\UDDigiKyokashoN-R.ttc', 'C:\\\\Windows\\\\Fonts\\\\yumindb.ttf', 'C:\\\\Windows\\\\Fonts\\\\mmrtext.ttf', 'C:\\\\Windows\\\\Fonts\\\\georgiab.ttf', 'C:\\\\Windows\\\\Fonts\\\\cour.ttf', 'C:\\\\Windows\\\\Fonts\\\\framdit.ttf', 'C:\\\\Windows\\\\Fonts\\\\cambriaz.ttf', 'C:\\\\Windows\\\\Fonts\\\\calibrib.ttf', 'C:\\\\Windows\\\\Fonts\\\\monbaiti.ttf', 'C:\\\\Windows\\\\Fonts\\\\msmincho.ttc', 'C:\\\\Windows\\\\Fonts\\\\meiryo.ttc', 'C:\\\\Windows\\\\Fonts\\\\comicbd.ttf', 'C:\\\\Windows\\\\Fonts\\\\cambria.ttc', 'C:\\\\Windows\\\\Fonts\\\\phagspa.ttf', 'C:\\\\Windows\\\\Fonts\\\\corbel.ttf', 'C:\\\\Windows\\\\Fonts\\\\LeelUIsl.ttf', 'C:\\\\Windows\\\\Fonts\\\\consolaz.ttf', 'C:\\\\Windows\\\\Fonts\\\\Candara.ttf', 'C:\\\\Windows\\\\Fonts\\\\ariali.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguisli.ttf', 'C:\\\\Windows\\\\Fonts\\\\msjh.ttc', 'C:\\\\Windows\\\\Fonts\\\\corbeli.ttf', 'C:\\\\Windows\\\\Fonts\\\\lucon.ttf', 'C:\\\\Windows\\\\Fonts\\\\arial.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoeui.ttf', 'C:\\\\Windows\\\\Fonts\\\\gadugi.ttf', 'C:\\\\Windows\\\\Fonts\\\\msjhbd.ttc', 'C:\\\\Windows\\\\Fonts\\\\verdanab.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoepr.ttf', 'C:\\\\Windows\\\\Fonts\\\\mingliub.ttc', 'C:\\\\Windows\\\\Fonts\\\\calibri.ttf', 'C:\\\\Windows\\\\Fonts\\\\calibriz.ttf', 'C:\\\\Windows\\\\Fonts\\\\Inkfree.ttf', 'C:\\\\Windows\\\\Fonts\\\\Candarai.ttf', 'C:\\\\Windows\\\\Fonts\\\\constani.ttf', 'C:\\\\Windows\\\\Fonts\\\\corbelb.ttf', 'C:\\\\Windows\\\\Fonts\\\\verdana.ttf', 'C:\\\\Windows\\\\Fonts\\\\CascadiaMono.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoeprb.ttf', 'C:\\\\Windows\\\\Fonts\\\\Candaral.ttf', 'C:\\\\Windows\\\\Fonts\\\\SitkaVF.ttf', 'C:\\\\Windows\\\\Fonts\\\\simsun.ttc', 'C:\\\\Windows\\\\Fonts\\\\wingding.ttf', 'C:\\\\Windows\\\\Fonts\\\\himalaya.ttf', 'C:\\\\Windows\\\\Fonts\\\\SimsunExtG.ttf', 'C:\\\\Windows\\\\Fonts\\\\Candarab.ttf', 'C:\\\\Windows\\\\Fonts\\\\SegUIVar.ttf', 'C:\\\\Windows\\\\Fonts\\\\impact.ttf', 'C:\\\\Windows\\\\Fonts\\\\O019000M.TTF', 'C:\\\\Windows\\\\Fonts\\\\couri.ttf', 'C:\\\\Windows\\\\Fonts\\\\yuminl.ttf', 'C:\\\\Windows\\\\Fonts\\\\bahnschrift.ttf', 'C:\\\\Windows\\\\Fonts\\\\YuGothB.ttc', 'C:\\\\Windows\\\\Fonts\\\\seguisb.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguili.ttf', 'C:\\\\Windows\\\\Fonts\\\\CascadiaCode.ttf', 'C:\\\\Windows\\\\Fonts\\\\trebucit.ttf', 'C:\\\\Windows\\\\Fonts\\\\taile.ttf', 'C:\\\\Windows\\\\Fonts\\\\times.ttf', 'C:\\\\Windows\\\\Fonts\\\\calibrili.ttf', 'C:\\\\Windows\\\\Fonts\\\\yumin.ttf', 'C:\\\\Windows\\\\Fonts\\\\tahomabd.ttf', 'C:\\\\Windows\\\\Fonts\\\\calibrii.ttf', 'C:\\\\Windows\\\\Fonts\\\\framd.ttf', 'C:\\\\Windows\\\\Fonts\\\\comic.ttf', 'C:\\\\Windows\\\\Fonts\\\\YuGothM.ttc', 'C:\\\\Windows\\\\Fonts\\\\BIZ-UDGothicR.ttc', 'C:\\\\Windows\\\\Fonts\\\\msyh.ttc', 'C:\\\\Windows\\\\Fonts\\\\segmdl2.ttf', 'C:\\\\Windows\\\\Fonts\\\\trebucbi.ttf', 'C:\\\\Windows\\\\Fonts\\\\SansSerifCollection.ttf', 'C:\\\\Windows\\\\Fonts\\\\timesi.ttf', 'C:\\\\Windows\\\\Fonts\\\\meiryob.ttc', 'C:\\\\Windows\\\\Fonts\\\\BIZ-UDMinchoM.ttc', 'C:\\\\Windows\\\\Fonts\\\\georgiaz.ttf', 'C:\\\\Windows\\\\Fonts\\\\verdanaz.ttf', 'C:\\\\Windows\\\\Fonts\\\\palabi.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguiemj.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoescb.ttf', 'C:\\\\Windows\\\\Fonts\\\\msjhl.ttc', 'C:\\\\Windows\\\\Fonts\\\\ntailub.ttf', 'C:\\\\Windows\\\\Fonts\\\\l_10646.ttf', 'C:\\\\Windows\\\\Fonts\\\\LeelaUIb.ttf', 'C:\\\\Windows\\\\Fonts\\\\YuGothL.ttc', 'C:\\\\Windows\\\\Fonts\\\\trebucbd.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoeuib.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguibl.ttf', 'C:\\\\Windows\\\\Fonts\\\\arialbd.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoeuisl.ttf', 'C:\\\\Windows\\\\Fonts\\\\webdings.ttf', 'C:\\\\Windows\\\\Fonts\\\\ntailu.ttf', 'C:\\\\Windows\\\\Fonts\\\\arialbi.ttf', 'C:\\\\Windows\\\\Fonts\\\\ebrima.ttf', 'C:\\\\Windows\\\\Fonts\\\\taileb.ttf', 'C:\\\\Windows\\\\Fonts\\\\ariblk.ttf', 'C:\\\\Windows\\\\Fonts\\\\simsunb.ttf', 'C:\\\\Windows\\\\Fonts\\\\cambriai.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguihis.ttf', 'C:\\\\Windows\\\\Fonts\\\\verdanai.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoesc.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguibli.ttf', 'C:\\\\Windows\\\\Fonts\\\\Candarali.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoeuil.ttf', 'C:\\\\Windows\\\\Fonts\\\\BIZ-UDGothicB.ttc', 'C:\\\\Windows\\\\Fonts\\\\consolai.ttf', 'C:\\\\Windows\\\\Fonts\\\\mvboli.ttf', 'C:\\\\Windows\\\\Fonts\\\\georgiai.ttf', 'C:\\\\Windows\\\\Fonts\\\\malgunsl.ttf', 'C:\\\\Windows\\\\Fonts\\\\trebuc.ttf', 'C:\\\\Windows\\\\Fonts\\\\comicz.ttf', 'C:\\\\Windows\\\\Fonts\\\\Nirmala.ttc', 'C:\\\\Windows\\\\Fonts\\\\msyhbd.ttc', 'C:\\\\Windows\\\\Fonts\\\\comici.ttf', 'C:\\\\Windows\\\\Fonts\\\\timesbd.ttf', 'C:\\\\Windows\\\\Fonts\\\\UDDigiKyokashoN-B.ttc', 'C:\\\\Windows\\\\Fonts\\\\palab.ttf', 'C:\\\\Windows\\\\Fonts\\\\constan.ttf', 'C:\\\\Windows\\\\Fonts\\\\courbd.ttf', 'C:\\\\Windows\\\\Fonts\\\\YuGothR.ttc', 'C:\\\\Windows\\\\Fonts\\\\msyi.ttf', 'C:\\\\Windows\\\\Fonts\\\\calibril.ttf', 'C:\\\\Windows\\\\Fonts\\\\msgothic.ttc', 'C:\\\\Windows\\\\Fonts\\\\cambriab.ttf', 'C:\\\\Windows\\\\Fonts\\\\constanb.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguisbi.ttf', 'C:\\\\Windows\\\\Fonts\\\\SegoeIcons.ttf', 'C:\\\\Windows\\\\Fonts\\\\msyhl.ttc', 'C:\\\\Windows\\\\Fonts\\\\consolab.ttf', 'C:\\\\Windows\\\\Fonts\\\\micross.ttf', 'C:\\\\Windows\\\\Fonts\\\\georgia.ttf', 'C:\\\\Windows\\\\Fonts\\\\Roboto-Regular.ttf', 'C:\\\\Windows\\\\Fonts\\\\corbell.ttf', 'C:\\\\Windows\\\\Fonts\\\\seguisym.ttf', 'C:\\\\Windows\\\\Fonts\\\\ebrimabd.ttf', 'C:\\\\Windows\\\\Fonts\\\\symbol.ttf', 'C:\\\\Windows\\\\Fonts\\\\courbi.ttf', 'C:\\\\Windows\\\\Fonts\\\\SitkaVF-Italic.ttf', 'C:\\\\Windows\\\\Fonts\\\\segoeuii.ttf', 'C:\\\\Windows\\\\Fonts\\\\phagspab.ttf', 'C:\\\\Windows\\\\Fonts\\\\sylfaen.ttf', 'C:\\\\Windows\\\\Fonts\\\\timesbi.ttf', 'C:\\\\Windows\\\\Fonts\\\\constanz.ttf', 'C:\\\\Windows\\\\Fonts\\\\palai.ttf', 'C:\\\\Windows\\\\Fonts\\\\Candaraz.ttf', 'C:\\\\Windows\\\\Fonts\\\\corbelz.ttf']\n",
      "個別推論処理を開始します...\n",
      "処理中: 000005406f824b6c8136e244c2199eb4.png\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be an integer with 1 <= num <= 3, not 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 138\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m test_files:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m処理中: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 138\u001b[0m     \u001b[43minfer_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# または、より効率的なバッチ処理で推論\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# print(\"バッチ推論処理を開始します...\")\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# batch_inference(batch_size=16)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m推論処理が完了しました。結果は以下のディレクトリに保存されています:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 116\u001b[0m, in \u001b[0;36minfer_single_image\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# plt.subplot(1, 3, 2)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# plt.title('生成結果', fontsize=14)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# plt.imshow(predicted_skin)\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# plt.axis('off')\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (key, predicted_skin) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(models_dir\u001b[38;5;241m.\u001b[39mkeys(), predicted_skins)):\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m生成結果 (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m14\u001b[39m)\n\u001b[0;32m    118\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(predicted_skin)\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\pyplot.py:1534\u001b[0m, in \u001b[0;36msubplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1531\u001b[0m fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;66;03m# First, search for an existing subplot with a matching spec.\u001b[39;00m\n\u001b[1;32m-> 1534\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mSubplotSpec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_subplot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m fig\u001b[38;5;241m.\u001b[39maxes:\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;66;03m# If we found an Axes at the position, we can reuse it if the user passed no\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;66;03m# kwargs or if the Axes class and kwargs are identical.\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (ax\u001b[38;5;241m.\u001b[39mget_subplotspec() \u001b[38;5;241m==\u001b[39m key\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (kwargs \u001b[38;5;241m==\u001b[39m {}\n\u001b[0;32m   1541\u001b[0m              \u001b[38;5;129;01mor\u001b[39;00m (ax\u001b[38;5;241m.\u001b[39m_projection_init\n\u001b[0;32m   1542\u001b[0m                  \u001b[38;5;241m==\u001b[39m fig\u001b[38;5;241m.\u001b[39m_process_projection_requirements(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)))):\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\matplotlib\\gridspec.py:589\u001b[0m, in \u001b[0;36mSubplotSpec._from_subplot_args\u001b[1;34m(figure, args)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num, Integral) \u001b[38;5;129;01mor\u001b[39;00m num \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m num \u001b[38;5;241m>\u001b[39m rows\u001b[38;5;241m*\u001b[39mcols:\n\u001b[1;32m--> 589\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum must be an integer with 1 <= num <= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrows\u001b[38;5;241m*\u001b[39mcols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    591\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    592\u001b[0m         )\n\u001b[0;32m    593\u001b[0m     i \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m=\u001b[39m num\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gs[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:j]\n",
      "\u001b[1;31mValueError\u001b[0m: num must be an integer with 1 <= num <= 3, not 5"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x3000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib as mpl\n",
    "\n",
    "# 日本語フォントの設定\n",
    "# Windows の場合\n",
    "plt.rcParams['font.family'] = 'MS Gothic'  # Windows 標準の日本語フォント\n",
    "# あるいは IPAフォントなどがインストール済みの場合\n",
    "# plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "# Mac の場合はコメントアウトを外す\n",
    "# plt.rcParams['font.family'] = 'Hiragino Sans GB'\n",
    "\n",
    "# Linux の場合はコメントアウトを外す\n",
    "# plt.rcParams['font.family'] = 'Noto Sans CJK JP'\n",
    "\n",
    "# ディレクトリパス設定\n",
    "skins_dir = 'C:/Users/Owner/Desktop/archive/Skins'\n",
    "missing_dir = 'C:/Users/Owner/Desktop/archive/Missing'\n",
    "masks_dir = 'C:/Users/Owner/Desktop/archive/Masks'\n",
    "output_dir = 'C:/Users/Owner/Desktop/archive/Results'\n",
    "\n",
    "# 出力ディレクトリが存在しない場合は作成\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 画像読み込み関数\n",
    "def load_image(path, channels=4):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=channels)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1]に正規化\n",
    "    return image\n",
    "\n",
    "# 推論用サンプルの準備\n",
    "def prepare_inference_sample(file_name):\n",
    "    # ファイル名からパスを生成\n",
    "    missing_file = tf.strings.join([\"missing_\", file_name])\n",
    "    missing_path = tf.strings.join([missing_dir, missing_file], separator='/')\n",
    "    mask_file = tf.strings.join([\"mask_\", file_name])\n",
    "    mask_path = tf.strings.join([masks_dir, mask_file], separator='/')\n",
    "    \n",
    "    # 画像とマスクを読み込み\n",
    "    missing = load_image(missing_path, channels=4)\n",
    "    mask = load_image(mask_path, channels=1)\n",
    "    \n",
    "    # 入力は欠損画像とマスクをチャネル方向に連結\n",
    "    input_image = tf.concat([missing, mask], axis=-1)\n",
    "    \n",
    "    # バッチ次元を追加\n",
    "    input_image = tf.expand_dims(input_image, 0)\n",
    "    \n",
    "    return input_image, file_name\n",
    "\n",
    "# 保存された学習済みモデルを読み込む\n",
    "# model = load_model('U_NET/UNET_50000.h5')\n",
    "# model = load_model('U_NET/UNET.h5', custom_objects={\"total_loss\": total_loss})\n",
    "models_dir = {}\n",
    "models_dir[\"l1\"] = load_model(\"models/l1_loss.h5\", custom_objects={\"l1_loss\": l1_loss})\n",
    "models_dir[\"l2\"] = load_model(\"models/l2_loss.h5\", custom_objects={\"l2_loss\": l2_loss})\n",
    "models_dir[\"edge\"] = load_model(\"models/edge_loss.h5\", custom_objects={\"edge_loss\": edge_loss})\n",
    "models_dir[\"mae\"] = load_model(\"models/mae_loss.h5\", custom_objects={\"mae_loss\": mae_loss})\n",
    "models_dir[\"mse\"] = load_model(\"models/mse_loss.h5\", custom_objects={\"mse_loss\": mse_loss})\n",
    "models_dir[\"ssim\"] = load_model(\"models/ssim_loss.h5\", custom_objects={\"ssim_loss\": ssim_loss})\n",
    "models_dir[\"total\"] = load_model(\"models/total_loss.h5\", custom_objects={\"total_loss\": total_loss})\n",
    "\n",
    "# テスト用画像ファイル名のリストを取得\n",
    "test_files = os.listdir(skins_dir)\n",
    "test_files = [f for f in test_files if f.endswith('.png')]\n",
    "\n",
    "# テスト画像数の制限（必要に応じて）\n",
    "test_limit = 30\n",
    "test_files = test_files[:test_limit]\n",
    "\n",
    "# 単一画像の推論を行う関数（特定の画像に対して処理したい場合）\n",
    "def infer_single_image(filename):\n",
    "    # 推論用サンプルを準備\n",
    "    input_image, _ = prepare_inference_sample(filename)\n",
    "    \n",
    "    # # 推論実行\n",
    "    # predicted_skin = model.predict(input_image)[0]\n",
    "\n",
    "    predicted_skins = []\n",
    "    for model in models_dir.values():\n",
    "        predicted_skins.append(model.predict(input_image)[0])\n",
    "    \n",
    "    # 元の画像も読み込む（比較用）\n",
    "    missing_path = os.path.join(missing_dir, f\"missing_{filename}\")\n",
    "    missing_image = load_image(missing_path).numpy()\n",
    "    \n",
    "    original_path = os.path.join(skins_dir, filename)\n",
    "    original_image = load_image(original_path).numpy()\n",
    "    \n",
    "    # 結果の可視化と表示\n",
    "    # plt.figure(figsize=(15, 5))\n",
    "    fig, axes = plt.subplots(1, 9, figsize=(18, 2))\n",
    "    \n",
    "    # 元の画像\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('元画像', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 欠損画像\n",
    "    axes[1].imshow(missing_image)\n",
    "    axes[1].set_title('欠損画像', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    for i, (key, predicted_skin) in enumerate(zip(models_dir.keys(), predicted_skins)):\n",
    "        axes[i+2].imshow(predicted_skin)\n",
    "        axes[i+2].set_title(f'{key}損失', fontsize=14)\n",
    "        axes[i+2].axis('off')\n",
    "    \n",
    "    # # タイトルがきちんと表示されるよう余白調整\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return predicted_skins\n",
    "\n",
    "# メイン実行部分\n",
    "if __name__ == \"__main__\":\n",
    "    # フォント設定の確認\n",
    "    print(\"利用可能なフォント:\")\n",
    "    print(mpl.font_manager.findSystemFonts(fontpaths=None, fontext='ttf'))\n",
    "\n",
    "    # 全てのテスト画像で推論を実行\n",
    "    print(\"個別推論処理を開始します...\")\n",
    "    for file_name in test_files:\n",
    "        print(f\"処理中: {file_name}\")\n",
    "        infer_single_image(file_name)\n",
    "    \n",
    "    # または、より効率的なバッチ処理で推論\n",
    "    # print(\"バッチ推論処理を開始します...\")\n",
    "    # batch_inference(batch_size=16)\n",
    "    \n",
    "    print(\"推論処理が完了しました。結果は以下のディレクトリに保存されています:\")\n",
    "    print(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
