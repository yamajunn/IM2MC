{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# 各フォルダパス\n",
    "skins_dir = 'C:/Users/Owner/Desktop/archive/Skins'\n",
    "missing_dir = 'C:/Users/Owner/Desktop/archive/Missing'\n",
    "masks_dir = 'C:/Users/Owner/Desktop/archive/Masks'\n",
    "\n",
    "def load_image(path, channels=4):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=channels)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1]に正規化\n",
    "    return image\n",
    "\n",
    "def load_sample(file_name):\n",
    "    # file_name は Tensor で、ファイル名のみが入っている前提\n",
    "    skin_path = tf.strings.join([skins_dir, file_name], separator='/')\n",
    "    \n",
    "    missing_file = tf.strings.join([\"missing_\", file_name])\n",
    "    missing_path = tf.strings.join([missing_dir, missing_file], separator='/')\n",
    "    \n",
    "    mask_file = tf.strings.join([\"mask_\", file_name])\n",
    "    mask_path = tf.strings.join([masks_dir, mask_file], separator='/')\n",
    "    \n",
    "    skin = load_image(skin_path, channels=4)\n",
    "    missing = load_image(missing_path, channels=4)\n",
    "    mask = load_image(mask_path, channels=1)  # マスクは1チャネル\n",
    "    \n",
    "    # 入力は欠損画像とマスクをチャネル方向に連結 (shape: (64, 64, 5))\n",
    "    input_image = tf.concat([missing, mask], axis=-1)\n",
    "    return input_image, skin\n",
    "\n",
    "# データセットのサイズ\n",
    "total = 10000\n",
    "\n",
    "# 全てのスキン画像のファイルリストを取得\n",
    "file_names = tf.data.Dataset.list_files(os.path.join(skins_dir, '*.png')).shuffle(buffer_size=total)\n",
    "file_names = file_names.take(total)\n",
    "\n",
    "# ファイル名の抽出とサンプル作成\n",
    "dataset = file_names.map(lambda fn: load_sample(tf.strings.split(fn, os.sep)[-1]))\n",
    "dataset = dataset.shuffle(buffer_size=total)\n",
    "\n",
    "# 全体の件数が1000件の場合、80%をトレーニング、20%を検証に利用\n",
    "train_size = int(total * 0.8)\n",
    "# トレーニングデータと検証データに分割\n",
    "train_dataset = dataset.take(train_size).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset   = dataset.skip(train_size).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "\n",
    "# Generator model\n",
    "def build_generator():\n",
    "    # Input: Missing image (64x64x4) concatenated with mask (64x64x1)\n",
    "    inp = layers.Input(shape=[64, 64, 5], name='input_combined')\n",
    "    \n",
    "    # Encoder (downsampling)\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (32x32)\n",
    "        downsample(128, 4),  # (16x16)\n",
    "        downsample(256, 4),  # (8x8)\n",
    "        downsample(512, 4),  # (4x4)\n",
    "        downsample(512, 4),  # (2x2)\n",
    "    ]\n",
    "    \n",
    "    # Decoder (upsampling)\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (4x4)\n",
    "        upsample(256, 4, apply_dropout=True),  # (8x8)\n",
    "        upsample(128, 4),  # (16x16)\n",
    "        upsample(64, 4),  # (32x32)\n",
    "    ]\n",
    "    \n",
    "    # Final output layer\n",
    "    last = layers.Conv2DTranspose(4, 4, strides=2, padding='same',\n",
    "                                 activation='tanh')  # (64x64)\n",
    "    \n",
    "    # Downsampling\n",
    "    skips = []\n",
    "    x = inp\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    \n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    # Upsampling and skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "    \n",
    "    # Final output\n",
    "    x = last(x)\n",
    "    \n",
    "    return models.Model(inputs=inp, outputs=x, name='generator')\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    # Input: Generated/real image (64x64x4) and conditional input (64x64x5)\n",
    "    inp_conditional = layers.Input(shape=[64, 64, 5], name='input_conditional')\n",
    "    inp_target = layers.Input(shape=[64, 64, 4], name='target_image')\n",
    "    \n",
    "    x = layers.Concatenate()([inp_conditional, inp_target])\n",
    "    \n",
    "    # PatchGAN discriminator\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding='same')(x)  # (32x32)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)  # (16x16)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)  # (8x8)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # Patch output\n",
    "    x = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
    "    \n",
    "    return models.Model(inputs=[inp_conditional, inp_target], outputs=x, name='discriminator')\n",
    "\n",
    "# Downsampling layer\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    result = models.Sequential()\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                            kernel_initializer=initializer, use_bias=False))\n",
    "    \n",
    "    if apply_batchnorm:\n",
    "        result.add(layers.BatchNormalization())\n",
    "    \n",
    "    result.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Upsampling layer\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    result = models.Sequential()\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                     kernel_initializer=initializer, use_bias=False))\n",
    "    \n",
    "    result.add(layers.BatchNormalization())\n",
    "    \n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "    \n",
    "    result.add(layers.ReLU())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def l1_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def l2_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def edge_loss(y_true, y_pred):\n",
    "    sobel_true = tf.image.sobel_edges(y_true)\n",
    "    sobel_pred = tf.image.sobel_edges(y_pred)\n",
    "\n",
    "    # X方向のエッジ差分\n",
    "    edge_x_loss = tf.abs(sobel_true[..., 0] - sobel_pred[..., 0])\n",
    "    # Y方向のエッジ差分\n",
    "    edge_y_loss = tf.abs(sobel_true[..., 1] - sobel_pred[..., 1])\n",
    "\n",
    "    # 両方のエッジの差分の平均を損失として使う\n",
    "    return tf.reduce_mean(edge_x_loss + edge_y_loss)\n",
    "\n",
    "def mae_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    # SSIMは[0, 1]範囲の画像に適用されるため、出力を[0, 1]に正規化\n",
    "    y_true = (y_true + 1.0) / 2.0  # RGBA画像などの場合、[-1, 1]の範囲から[0, 1]に変換\n",
    "    y_pred = (y_pred + 1.0) / 2.0  # 同様に出力を[0, 1]に正規化\n",
    "    \n",
    "    # SSIMを計算\n",
    "    return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
    "\n",
    "def laplacian_filter(image):\n",
    "    \"\"\"RGBAの各チャンネルにラプラシアンフィルタを適用\"\"\"\n",
    "    laplacian_kernel = tf.constant([\n",
    "        [0,  1,  0],\n",
    "        [1, -4,  1],\n",
    "        [0,  1,  0]\n",
    "    ], dtype=tf.float32)\n",
    "    \n",
    "    laplacian_kernel = tf.reshape(laplacian_kernel, [3, 3, 1, 1])  # (高さ, 幅, 入力チャンネル, 出力チャンネル)\n",
    "    \n",
    "    # RGBAの各チャンネルに適用するためのフィルタを作成\n",
    "    filters = tf.tile(laplacian_kernel, [1, 1, 4, 1])  # (3, 3, 4, 4) に拡張\n",
    "\n",
    "    # 4次元テンソル (バッチ, 高さ, 幅, チャンネル) の形状を維持\n",
    "    image = tf.expand_dims(image, axis=0)  # バッチ次元を追加 (None, H, W, 4)\n",
    "    edges = tf.nn.conv2d(image, filters, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "\n",
    "    return tf.squeeze(edges)  # バッチ次元を削除\n",
    "\n",
    "def laplacian_loss(y_true, y_pred):\n",
    "    edge_true = laplacian_filter(y_true)\n",
    "    edge_pred = laplacian_filter(y_pred)\n",
    "\n",
    "    # L1 損失\n",
    "    loss = tf.reduce_mean(tf.abs(edge_true - edge_pred))\n",
    "    return loss\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    loss_l1 = l1_loss(y_true, y_pred)\n",
    "    # loss_edge = edge_loss(y_true, y_pred)\n",
    "    # loss_mae = mae_loss(y_true, y_pred)\n",
    "    # loss_mse = mse_loss(y_true, y_pred)\n",
    "    # loss_ssim = ssim_loss(y_true, y_pred)\n",
    "    loss_laplacian = laplacian_loss(y_true, y_pred)\n",
    "\n",
    "    return loss_l1 + 0.2 * loss_laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def generator_loss(disc_generated_output, gen_output, target, lambda_l1=100):\n",
    "    # Adversarial loss\n",
    "    gan_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # L1 loss (pixel-wise difference)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    loss_ssim = ssim_loss(target, gen_output)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = gan_loss + (lambda_l1 * l1_loss) + loss_ssim\n",
    "    \n",
    "    return total_loss, gan_loss, l1_loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    # Real image loss\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.ones_like(disc_real_output), disc_real_output)\n",
    "    \n",
    "    # Generated image loss\n",
    "    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Pix2Pix GAN model class (改善版)\n",
    "class Pix2PixGAN:\n",
    "    def __init__(self, lambda_l1=100):\n",
    "        self.generator = build_generator()\n",
    "        self.discriminator = build_discriminator()\n",
    "        \n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.lambda_l1 = lambda_l1\n",
    "\n",
    "        # 統計用のメトリクス\n",
    "        self.gen_total_loss_metric = tf.keras.metrics.Mean(name='gen_total_loss')\n",
    "        self.gen_gan_loss_metric = tf.keras.metrics.Mean(name='gen_gan_loss')\n",
    "        self.gen_l1_loss_metric = tf.keras.metrics.Mean(name='gen_l1_loss')\n",
    "        self.disc_loss_metric = tf.keras.metrics.Mean(name='disc_loss')\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, input_conditional, target):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # 生成画像の生成\n",
    "            gen_output = self.generator(input_conditional, training=True)\n",
    "            \n",
    "            # 識別器の予測\n",
    "            disc_real_output = self.discriminator([input_conditional, target], training=True)\n",
    "            disc_generated_output = self.discriminator([input_conditional, gen_output], training=True)\n",
    "            \n",
    "            # 損失計算\n",
    "            gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n",
    "                disc_generated_output, gen_output, target, self.lambda_l1)\n",
    "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        \n",
    "        # 勾配計算\n",
    "        generator_gradients = gen_tape.gradient(gen_total_loss, self.generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        \n",
    "        # 勾配適用\n",
    "        self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # メトリクス更新\n",
    "        self.gen_total_loss_metric(gen_total_loss)\n",
    "        self.gen_gan_loss_metric(gen_gan_loss)\n",
    "        self.gen_l1_loss_metric(gen_l1_loss)\n",
    "        self.disc_loss_metric(disc_loss)\n",
    "        \n",
    "        return gen_total_loss, disc_loss\n",
    "    \n",
    "    def fit(self, train_dataset, val_dataset=None, epochs=50):\n",
    "        # 事前に tf.data パイプラインの最適化（cache, prefetchなど）をしておくと良い\n",
    "        train_dataset = train_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        if val_dataset is not None:\n",
    "            val_dataset = val_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            clear_output(wait=True)\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            # メトリクスのリセット\n",
    "            self.gen_total_loss_metric.reset_states()\n",
    "            self.gen_gan_loss_metric.reset_states()\n",
    "            self.gen_l1_loss_metric.reset_states()\n",
    "            self.disc_loss_metric.reset_states()\n",
    "            \n",
    "            # トレーニングループ\n",
    "            for batch, (input_conditional, target) in enumerate(train_dataset):\n",
    "                gen_loss, disc_loss = self.train_step(input_conditional, target)\n",
    "                \n",
    "                if batch % 10 == 0:\n",
    "                    print(f'Batch {batch}: Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}')\n",
    "            \n",
    "            # エポックごとのメトリクス出力\n",
    "            print('Epoch {} Training Losses:'.format(epoch+1))\n",
    "            print(f'  gen_total_loss: {self.gen_total_loss_metric.result():.4f}')\n",
    "            print(f'  gen_gan_loss  : {self.gen_gan_loss_metric.result():.4f}')\n",
    "            print(f'  gen_l1_loss   : {self.gen_l1_loss_metric.result():.4f}')\n",
    "            print(f'  disc_loss     : {self.disc_loss_metric.result():.4f}')\n",
    "            \n",
    "            # バリデーション (ある場合)\n",
    "            if val_dataset is not None:\n",
    "                val_l1_metric = tf.keras.metrics.Mean(name='val_gen_l1_loss')\n",
    "                for input_conditional, target in val_dataset:\n",
    "                    gen_output = self.generator(input_conditional, training=False)\n",
    "                    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "                    val_l1_metric(l1_loss)\n",
    "                print(f'Epoch {epoch+1} Validation Loss: gen_l1_loss: {val_l1_metric.result():.4f}')\n",
    "            \n",
    "            # 10エポックごとのモデル保存とサンプル生成\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.generator.save(f'../../models/GAN/checkpoint/pixelart_inpainting_generator_epoch_{epoch+1}.h5')\n",
    "                if val_dataset is not None:\n",
    "                    self.generate_samples(val_dataset, epoch + 1)\n",
    "    \n",
    "    def generate_samples(self, dataset, epoch, num_samples=4):\n",
    "        # サンプル可視化部分はEagerモードで十分\n",
    "        for input_conditional, target in dataset.take(1):\n",
    "            input_samples = input_conditional[:num_samples]\n",
    "            target_samples = target[:num_samples]\n",
    "            predicted_samples = self.generator(input_samples, training=False)\n",
    "            \n",
    "            # conditional input から missing image と mask を分割\n",
    "            missing_samples = input_samples[:, :, :, :4]  # 最初の4チャンネル\n",
    "            mask_samples = input_samples[:, :, :, 4:5]    # 最後のチャンネル\n",
    "            \n",
    "            plt.figure(figsize=(15, 4 * num_samples))\n",
    "            for i in range(num_samples):\n",
    "                # Missing image\n",
    "                plt.subplot(num_samples, 4, i * 4 + 1)\n",
    "                plt.imshow(missing_samples[i])\n",
    "                plt.title(\"Missing\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "                # Mask\n",
    "                plt.subplot(num_samples, 4, i * 4 + 2)\n",
    "                plt.imshow(tf.squeeze(mask_samples[i]), cmap='gray')\n",
    "                plt.title(\"Mask\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "                # Generated image\n",
    "                plt.subplot(num_samples, 4, i * 4 + 3)\n",
    "                plt.imshow(predicted_samples[i] * 0.5 + 0.5)  # Denormalize\n",
    "                plt.title(\"Generated\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "                # Target image\n",
    "                plt.subplot(num_samples, 4, i * 4 + 4)\n",
    "                plt.imshow(target_samples[i])\n",
    "                plt.title(\"Target\")\n",
    "                plt.axis(\"off\")\n",
    "            \n",
    "            plt.savefig(f'../../Output/GAN/samples_epoch_{epoch}.png')\n",
    "            plt.close()\n",
    "            break  # 一度のバッチのみ処理\n",
    "    \n",
    "    def inpaint(self, missing_image, mask):\n",
    "        # 入力画像にバッチ次元を追加\n",
    "        if len(missing_image.shape) == 3:\n",
    "            missing_image = tf.expand_dims(missing_image, 0)\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = tf.expand_dims(mask, 0)\n",
    "        \n",
    "        # 条件入力作成\n",
    "        input_conditional = tf.concat([missing_image, mask], axis=-1)\n",
    "        generated = self.generator(input_conditional, training=False)\n",
    "        return tf.squeeze(generated, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Pix2PixGAN(lambda_l1=100)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, val_dataset, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generator.save('../../models/GAN/GAN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
