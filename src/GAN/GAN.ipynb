{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# 各フォルダパス\n",
    "skins_dir = 'C:/Users/Owner/Desktop/archive/Skins'\n",
    "missing_dir = 'C:/Users/Owner/Desktop/archive/Missing'\n",
    "masks_dir = 'C:/Users/Owner/Desktop/archive/Masks'\n",
    "\n",
    "def load_image(path, channels=4):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=channels)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1]に正規化\n",
    "    return image\n",
    "\n",
    "def load_sample(file_name):\n",
    "    # file_name は Tensor で、ファイル名のみが入っている前提\n",
    "    skin_path = tf.strings.join([skins_dir, file_name], separator='/')\n",
    "    \n",
    "    missing_file = tf.strings.join([\"missing_\", file_name])\n",
    "    missing_path = tf.strings.join([missing_dir, missing_file], separator='/')\n",
    "    \n",
    "    mask_file = tf.strings.join([\"mask_\", file_name])\n",
    "    mask_path = tf.strings.join([masks_dir, mask_file], separator='/')\n",
    "    \n",
    "    skin = load_image(skin_path, channels=4)\n",
    "    missing = load_image(missing_path, channels=4)\n",
    "    mask = load_image(mask_path, channels=1)  # マスクは1チャネル\n",
    "    \n",
    "    # 入力は欠損画像とマスクをチャネル方向に連結 (shape: (64, 64, 5))\n",
    "    input_image = tf.concat([missing, mask], axis=-1)\n",
    "    return input_image, skin\n",
    "\n",
    "# データセットのサイズ\n",
    "total = 1000\n",
    "\n",
    "# 全てのスキン画像のファイルリストを取得\n",
    "file_names = tf.data.Dataset.list_files(os.path.join(skins_dir, '*.png')).shuffle(buffer_size=total)\n",
    "file_names = file_names.take(total)\n",
    "\n",
    "# ファイル名の抽出とサンプル作成\n",
    "dataset = file_names.map(lambda fn: load_sample(tf.strings.split(fn, os.sep)[-1]))\n",
    "dataset = dataset.shuffle(buffer_size=total)\n",
    "\n",
    "# 全体の件数が1000件の場合、80%をトレーニング、20%を検証に利用\n",
    "train_size = int(total * 0.8)\n",
    "# トレーニングデータと検証データに分割\n",
    "train_dataset = dataset.take(train_size).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset   = dataset.skip(train_size).batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "\n",
    "# Generator model\n",
    "def build_generator():\n",
    "    # Input: Missing image (64x64x4) concatenated with mask (64x64x1)\n",
    "    inp = layers.Input(shape=[64, 64, 5], name='input_combined')\n",
    "    \n",
    "    # Encoder (downsampling)\n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (32x32)\n",
    "        downsample(128, 4),  # (16x16)\n",
    "        downsample(256, 4),  # (8x8)\n",
    "        downsample(512, 4),  # (4x4)\n",
    "        downsample(512, 4),  # (2x2)\n",
    "    ]\n",
    "    \n",
    "    # Decoder (upsampling)\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (4x4)\n",
    "        upsample(256, 4, apply_dropout=True),  # (8x8)\n",
    "        upsample(128, 4),  # (16x16)\n",
    "        upsample(64, 4),  # (32x32)\n",
    "    ]\n",
    "    \n",
    "    # Final output layer\n",
    "    last = layers.Conv2DTranspose(4, 4, strides=2, padding='same',\n",
    "                                 activation='tanh')  # (64x64)\n",
    "    \n",
    "    # Downsampling\n",
    "    skips = []\n",
    "    x = inp\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    \n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    # Upsampling and skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "    \n",
    "    # Final output\n",
    "    x = last(x)\n",
    "    \n",
    "    return models.Model(inputs=inp, outputs=x, name='generator')\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator():\n",
    "    # Input: Generated/real image (64x64x4) and conditional input (64x64x5)\n",
    "    inp_conditional = layers.Input(shape=[64, 64, 5], name='input_conditional')\n",
    "    inp_target = layers.Input(shape=[64, 64, 4], name='target_image')\n",
    "    \n",
    "    x = layers.Concatenate()([inp_conditional, inp_target])\n",
    "    \n",
    "    # PatchGAN discriminator\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding='same')(x)  # (32x32)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)  # (16x16)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)  # (8x8)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    \n",
    "    # Patch output\n",
    "    x = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n",
    "    \n",
    "    return models.Model(inputs=[inp_conditional, inp_target], outputs=x, name='discriminator')\n",
    "\n",
    "# Downsampling layer\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    result = models.Sequential()\n",
    "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                            kernel_initializer=initializer, use_bias=False))\n",
    "    \n",
    "    if apply_batchnorm:\n",
    "        result.add(layers.BatchNormalization())\n",
    "    \n",
    "    result.add(layers.LeakyReLU(0.2))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Upsampling layer\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    result = models.Sequential()\n",
    "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                                     kernel_initializer=initializer, use_bias=False))\n",
    "    \n",
    "    result.add(layers.BatchNormalization())\n",
    "    \n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "    \n",
    "    result.add(layers.ReLU())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def generator_loss(disc_generated_output, gen_output, target, lambda_l1=100):\n",
    "    # Adversarial loss\n",
    "    gan_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # L1 loss (pixel-wise difference)\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = gan_loss + (lambda_l1 * l1_loss)\n",
    "    \n",
    "    return total_loss, gan_loss, l1_loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    # Real image loss\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.ones_like(disc_real_output), disc_real_output)\n",
    "    \n",
    "    # Generated image loss\n",
    "    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(\n",
    "        tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = real_loss + generated_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Pix2Pix GAN model class\n",
    "class Pix2PixGAN:\n",
    "    def __init__(self, lambda_l1=100):\n",
    "        self.generator = build_generator()\n",
    "        self.discriminator = build_discriminator()\n",
    "        \n",
    "        self.generator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.discriminator_optimizer = optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.lambda_l1 = lambda_l1\n",
    "    \n",
    "    # Training step\n",
    "    @tf.function\n",
    "    def train_step(self, input_conditional, target):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # Generate output\n",
    "            gen_output = self.generator(input_conditional, training=True)\n",
    "            \n",
    "            # Discriminator predictions\n",
    "            disc_real_output = self.discriminator([input_conditional, target], training=True)\n",
    "            disc_generated_output = self.discriminator([input_conditional, gen_output], training=True)\n",
    "            \n",
    "            # Calculate losses\n",
    "            gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(\n",
    "                disc_generated_output, gen_output, target, self.lambda_l1)\n",
    "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        generator_gradients = gen_tape.gradient(\n",
    "            gen_total_loss, self.generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(\n",
    "            disc_loss, self.discriminator.trainable_variables)\n",
    "        \n",
    "        # Apply gradients\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            'gen_total_loss': gen_total_loss,\n",
    "            'gen_gan_loss': gen_gan_loss,\n",
    "            'gen_l1_loss': gen_l1_loss,\n",
    "            'disc_loss': disc_loss\n",
    "        }\n",
    "    \n",
    "    # Model training\n",
    "    def fit(self, train_dataset, val_dataset=None, epochs=50):\n",
    "        for epoch in range(epochs):\n",
    "            clear_output()\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            # Track losses for each epoch\n",
    "            train_losses = {\n",
    "                'gen_total_loss': [],\n",
    "                'gen_gan_loss': [],\n",
    "                'gen_l1_loss': [],\n",
    "                'disc_loss': []\n",
    "            }\n",
    "            \n",
    "            # Training loop\n",
    "            for batch, (input_conditional, target) in enumerate(train_dataset):\n",
    "                losses = self.train_step(input_conditional, target)\n",
    "                \n",
    "                for k, v in losses.items():\n",
    "                    train_losses[k].append(v.numpy())\n",
    "                \n",
    "                if batch % 10 == 0:\n",
    "                    print(f'Batch {batch}: Gen Loss: {losses[\"gen_total_loss\"]:.4f}, Disc Loss: {losses[\"disc_loss\"]:.4f}')\n",
    "            \n",
    "            # Print average losses for the epoch\n",
    "            print(f'Epoch {epoch+1} Training Losses:')\n",
    "            for k, v in train_losses.items():\n",
    "                print(f'  {k}: {np.mean(v):.4f}')\n",
    "            \n",
    "            # Validation (if provided)\n",
    "            if val_dataset is not None:\n",
    "                val_losses = {\n",
    "                    'gen_total_loss': [],\n",
    "                    'gen_l1_loss': []\n",
    "                }\n",
    "                \n",
    "                for input_conditional, target in val_dataset:\n",
    "                    # Generate predictions\n",
    "                    gen_output = self.generator(input_conditional, training=False)\n",
    "                    \n",
    "                    # Calculate L1 loss for validation\n",
    "                    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "                    val_losses['gen_l1_loss'].append(l1_loss.numpy())\n",
    "                \n",
    "                # Print validation losses\n",
    "                print(f'Epoch {epoch+1} Validation Losses:')\n",
    "                for k, v in val_losses.items():\n",
    "                    print(f'  {k}: {np.mean(v):.4f}')\n",
    "            \n",
    "            # Save model every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                self.generator.save(f'../../models/GAN/checkpoint/pixelart_inpainting_generator_epoch_{epoch+1}.h5')\n",
    "                \n",
    "                # Generate and save sample images\n",
    "                if val_dataset is not None:\n",
    "                    self.generate_samples(val_dataset, epoch + 1)\n",
    "    \n",
    "    # Generate and save sample images\n",
    "    def generate_samples(self, dataset, epoch, num_samples=4):\n",
    "        for input_conditional, target in dataset.take(1):\n",
    "            # Only use a few samples for visualization\n",
    "            input_samples = input_conditional[:num_samples]\n",
    "            target_samples = target[:num_samples]\n",
    "            \n",
    "            # Generate predictions\n",
    "            predicted_samples = self.generator(input_samples, training=False)\n",
    "            \n",
    "            # Extract missing images and masks from the conditional input\n",
    "            missing_samples = input_samples[:, :, :, :4]  # First 4 channels\n",
    "            mask_samples = input_samples[:, :, :, 4:5]    # Last channel\n",
    "            \n",
    "            # Create figure\n",
    "            plt.figure(figsize=(15, 4 * num_samples))\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                # Missing image\n",
    "                plt.subplot(num_samples, 4, i * 4 + 1)\n",
    "                plt.imshow(missing_samples[i])\n",
    "                plt.title(\"Missing\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "                # Mask\n",
    "                plt.subplot(num_samples, 4, i * 4 + 2)\n",
    "                plt.imshow(tf.squeeze(mask_samples[i]), cmap='gray')\n",
    "                plt.title(\"Mask\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "                # Generated image\n",
    "                plt.subplot(num_samples, 4, i * 4 + 3)\n",
    "                plt.imshow(predicted_samples[i] * 0.5 + 0.5)  # Denormalize\n",
    "                plt.title(\"Generated\")\n",
    "                plt.axis(\"off\")\n",
    "                \n",
    "                # Target image\n",
    "                plt.subplot(num_samples, 4, i * 4 + 4)\n",
    "                plt.imshow(target_samples[i])\n",
    "                plt.title(\"Target\")\n",
    "                plt.axis(\"off\")\n",
    "            \n",
    "            plt.savefig(f'../../Output/GAN/samples_epoch_{epoch}.png')\n",
    "            plt.close()\n",
    "            break  # Only process one batch\n",
    "    \n",
    "    # Perform inpainting on new images\n",
    "    def inpaint(self, missing_image, mask):\n",
    "        # Ensure correct shapes and types\n",
    "        if len(missing_image.shape) == 3:  # Add batch dimension if needed\n",
    "            missing_image = tf.expand_dims(missing_image, 0)\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = tf.expand_dims(mask, 0)\n",
    "        \n",
    "        # Create conditional input\n",
    "        input_conditional = tf.concat([missing_image, mask], axis=-1)\n",
    "        \n",
    "        # Generate inpainted image\n",
    "        generated = self.generator(input_conditional, training=False)\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        generated = tf.squeeze(generated, 0)\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Pix2PixGAN(lambda_l1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 65\u001b[0m, in \u001b[0;36mPix2PixGAN.fit\u001b[1;34m(self, train_dataset, val_dataset, epochs)\u001b[0m\n\u001b[0;32m     57\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_total_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_gan_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen_l1_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [],\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisc_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: []\n\u001b[0;32m     62\u001b[0m }\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (input_conditional, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[0;32m     66\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(input_conditional, target)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3011\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3010\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3011\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3012\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3013\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3015\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = Pix2PixGAN(lambda_l1=100)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, val_dataset, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.generator.save('../../models/GAN/GAN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
